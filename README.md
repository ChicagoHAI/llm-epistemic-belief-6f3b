# Do LLMs Differentiate Epistemic Belief from Non-Epistemic Belief?

**Autonomous research experiment generated by [Idea Explorer](https://github.com/ChicagoHAI/idea-explorer)**

## Research Question

Large Language Models (LLMs) can distinguish between epistemic beliefs (beliefs about what is true or likely true) and non-epistemic beliefs (such as preferences, commitments, or pragmatic stances) when prompted with scenarios that require such differentiation. The models' responses will systematically reflect this distinction above chance and in ways that partially align with human judgments.


## Domain

Artificial Intelligence

## Overview

This repository contains an autonomous research experiment where an AI agent:
- Designed the experimental methodology
- Implemented all code
- Ran analyses
- Generated documentation

## Repository Structure

```
notebooks/          # Jupyter notebooks with experiments
  plan_Md.ipynb            # Research plan
  documentation_Md.ipynb   # Results and analysis
  code_walk_Md.ipynb       # Code walkthrough

results/            # Experimental outputs
  metrics.json             # Quantitative results
  *.png                    # Visualizations

.idea-explorer/     # Metadata
  idea.yaml                # Original idea specification
```

## Methodology

Controlled prompt-based probing and comparative analysis with human data

## Expected Outputs

- **Metrics**: Structured metrics for belief type classification by model and scenario
- **Visualization**: Plots showing:
1. Model vs. human accuracy on epistemic/non-epistemic belief distinction
2. Confusion matrices for each model
3. Agreement scores (Cohen's kappa) between models and humans

- **Analysis**: Qualitative analysis of LLM explanations, error types, and representative cases where models succeed or fail to distinguish belief types.

- **Report**: Comprehensive report including background, methodology, quantitative and qualitative results, discussion of implications for LLM theory-of-mind, and recommendations for future research.

- **Dataset**: Annotated scenario set with ground-truth belief type labels and corresponding LLM and human responses.



## Evaluation Criteria

- LLMs achieve statistically significant accuracy above random and majority-class baselines (p < 0.05)
- Agreement with human judgments (Cohen's kappa > 0.4) on at least one model
- Qualitative analysis identifies systematic patterns in model errors
- Results are reproducible across at least two LLMs
- All code and data are documented and released for reproducibility
- Experiments completed within specified compute and budget constraints


## How to Reproduce

1. Review the research plan: `notebooks/plan_Md.ipynb`
2. Follow the code walkthrough: `notebooks/code_walk_Md.ipynb`
3. Execute notebooks in order
4. Compare your results with `results/`

## Citation

If you use this research, please cite:

```bibtex
@misc{{idea_explorer_do_llms_differentiate_epistemi_20251103_230522_3d891818},
  title={{Do LLMs Differentiate Epistemic Belief from Non-Epistemic Belief?}},
  author={{Idea Explorer (Autonomous Agent)}},
  year={{2025}},
  url={{https://github.com/ChicagoHAI/do_llms_differentiate_epistemi_20251103_230522_3d891818}}
}
```

---

**Generated**: 2025-11-03 23:05:27
**Organization**: [ChicagoHAI](https://github.com/ChicagoHAI)
**Framework**: [Idea Explorer](https://github.com/ChicagoHAI/idea-explorer)
